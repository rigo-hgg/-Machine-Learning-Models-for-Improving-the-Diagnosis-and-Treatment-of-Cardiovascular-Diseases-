{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" prediction of plaque presence + feature selection 3.ipynb","provenance":[{"file_id":"1BSgO-s9ox3SBbQs7LNbQ98gfRhoPNCAj","timestamp":1645624974465}],"collapsed_sections":[],"authorship_tag":"ABX9TyN4XsEBcOTL3pJgr2WKzweC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lObgofx5RlSa"},"outputs":[],"source":["import math\n","import pandas as pd \n","import numpy as np\n","import imblearn\n","from sklearn import preprocessing \n","from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn import metrics\n","from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report, make_scorer, plot_roc_curve, recall_score, accuracy_score, precision_score\n","from sklearn.metrics import matthews_corrcoef \n","from collections import Counter\n","from imblearn.pipeline import Pipeline\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, StratifiedShuffleSplit\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier \n","from xgboost import XGBClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_classif, mutual_info_classif"]},{"cell_type":"code","source":["!pip install bootstrapped"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n42nXvPkGIpr","executionInfo":{"status":"ok","timestamp":1645625048988,"user_tz":-60,"elapsed":12247,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"5328a14d-ad5c-489f-a834-090fb8c35223"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bootstrapped\n","  Downloading bootstrapped-0.0.2.tar.gz (11 kB)\n","Requirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.7/dist-packages (from bootstrapped) (3.2.2)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from bootstrapped) (1.21.5)\n","Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from bootstrapped) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->bootstrapped) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->bootstrapped) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->bootstrapped) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->bootstrapped) (1.3.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.1->bootstrapped) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.3->bootstrapped) (1.15.0)\n","Building wheels for collected packages: bootstrapped\n","  Building wheel for bootstrapped (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bootstrapped: filename=bootstrapped-0.0.2-py2.py3-none-any.whl size=13954 sha256=4af20db3e35c43af73ddc48b3c83de2e1454dd07577cc83fad1e85c23db50ba2\n","  Stored in directory: /root/.cache/pip/wheels/15/55/6a/9a722f067ac4c3dfab359ed2ec7906b9cc6649156d9886bd59\n","Successfully built bootstrapped\n","Installing collected packages: bootstrapped\n","Successfully installed bootstrapped-0.0.2\n"]}]},{"cell_type":"code","source":["import bootstrapped.bootstrap as bs\n","import bootstrapped.stats_functions as bs_stats"],"metadata":{"id":"0iRpAIp3_vuF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":73},"id":"NyA4ForSSAWF","executionInfo":{"status":"ok","timestamp":1645625079847,"user_tz":-60,"elapsed":25187,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"a5aafeee-374b-4c94-d055-7e0577f123df"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-26ba3955-3c07-406b-b9e2-164cb20ca1c9\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-26ba3955-3c07-406b-b9e2-164cb20ca1c9\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving df.xlsx to df.xlsx\n"]}]},{"cell_type":"code","source":["df = pd.read_excel('df.xlsx')"],"metadata":{"id":"eH7LEq6FSdJ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X=df.loc[:,['_1' in i for i in df.columns]]"],"metadata":{"id":"6-b_EGy1Skw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = X.drop([\"visit:data_visita_1\", 'data_prelievo_1','ult_tsa:U_TSA_data_1',    #datetime variables\n","            'ana:istruzione_1',\n","            'ult_tsa:placca_dx_recod_1', 'ult_tsa:placca_sx_recod_1', 'ult_tsa:placca_1','ult_tsa:IMT_CC_max_sx_1', 'ult_tsa:IMT_CC_max_dx_1', 'ult_tsa:IMT_CC_medio_round_sx_1', 'ult_tsa:IMT_CC_medio_round_dx_1', 'ult_tsa:IMT_CC_medio_round_mean_1',  # variables regarding the presence of IMT and plaques\n","       'TG_cut_off_1', 'HDL_cut_off_1' , 'glucosio_cut_off_1' , 'pressione_cut_off_1', 'vita_guidelines_cut_off_1'  , 'vita_non_guidelines_cut_off_1' , 'sum_determinanti_guidelines_1',  'sum_determinanti_non_guidelines_1',  'mancanti_guidelines_1', 'mancanti_non_guidelines_1',   #other useless variables   \n","       'ana_pat:evento_1' ], axis=1)   #useless because at the time of the first visit no one has suffered from a cardiovascular event"],"metadata":{"id":"rFJ857pjSo0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y=df['ult_tsa:placca_tot']\n","# summarize class distribution\n","counter = Counter(y)\n","print(counter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2L0BXwfqSwu2","executionInfo":{"status":"ok","timestamp":1645625131062,"user_tz":-60,"elapsed":207,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"e6b357c0-1a7b-4454-d962-bccab2d46880"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({0: 567, 1: 380})\n"]}]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=42, stratify=y)\n","\n","\n","num_pipeline = Pipeline([\n","        ('scaler', StandardScaler()),\n","    ])\n","\n","cat_pipeline = Pipeline([\n","        ('ohe', OneHotEncoder(handle_unknown = 'ignore')),\n","    ])\n","    \n","num_attribs = list( X_train.select_dtypes(include=['int64', 'float64']).columns)\n","cat_attribs = list( X_train.select_dtypes(include='object').columns)\n","\n","preprocessor = ColumnTransformer([\n","        (\"num\", num_pipeline, num_attribs),\n","        (\"cat\", cat_pipeline, cat_attribs),\n","    ])"],"metadata":{"id":"U-mGmOdoS3-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selector = SelectKBest(score_func=mutual_info_classif, k='all')\n","selector_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n","                      ('selector', selector)]) "],"metadata":{"id":"_pc0n1NsTIhm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mcc_results=[]"],"metadata":{"id":"NQzj3P7Bq9sA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DECISION TREE CLASSIFIER"],"metadata":{"id":"7JU7EtlsTZkE"}},{"cell_type":"code","source":["metrics_df_tree = pd.DataFrame(columns=[\"Iteration\", \"Fold\", \"Features\" , \"Recall\", \"Precision\", 'MCC'])"],"metadata":{"id":"A1Fh2Pe-xsps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tree_param = {'model__max_leaf_nodes': list(range(2, 50)), 'model__min_samples_split': [2, 3, 4]}\n","\n","steps = [('preprocessor', preprocessor), ('model', DecisionTreeClassifier(random_state=42))]\n","tree_pipe = Pipeline(steps=steps)\n","\n","\n","\n","scorer = make_scorer(matthews_corrcoef)"],"metadata":{"id":"VX0aHLDfx2Mx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","steps=[10, 25, 50, preprocessor.fit_transform(X_train).shape[1]] #None to have all the features selected \n","n_ext=10\n","n_cv=5\n","ranking_tree = np.empty((n_ext * n_cv, preprocessor.fit_transform(X_train).shape[1]), dtype=int)\n","\n","for n in range(n_ext):\n","    \n","    skf = StratifiedKFold(n_cv, shuffle=True, random_state=n)\n","    \n","    for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n","        X_train_int, X_test_int = X_train.iloc[train_index], X_train.iloc[test_index]\n","        y_train_int, y_test_int = y_train.iloc[train_index], y_train.iloc[test_index]\n","        \n","        tuncv = StratifiedShuffleSplit(\n","                n_splits=n_cv, test_size=0.5, random_state=i\n","            )\n","        tree_grid = GridSearchCV(estimator = tree_pipe, param_grid = tree_param, scoring=scorer, cv=tuncv ,n_jobs=-1, verbose=False)\n","\n","    \n","    \n","        tree_grid.fit(X_train_int ,y_train_int) #GridSearchCV over training  fold to find the optimal parameters \n","\n","        best_model = tree_grid.best_estimator_.get_params()['model']\n","  \n","        selector_pipe.fit(X_train_int, y_train_int) #feature ranking through SelectKBest\n","\n","        #ordered list of tuples containing the index of the feature and its ranking:\n","        scores=selector.scores_[selector.get_support()]\n","        #feature=list(zip(range(scores.shape[0]), scores))\n","        #feature.sort(key=lambda x:x[1])\n","        ranking_tmp = np.argsort(scores)[::-1]\n","        ranking_tree[(n * n_cv) + i] = ranking_tmp\n","\n","    \n","        # rescaling\n","       \n","     \n","        X_train_int = preprocessor.fit_transform(X_train_int)\n","        X_test_int = preprocessor.transform(X_test_int)\n","        \n","\n","        #for step in steps:\n","          #selected_features=[tupla[0] for tupla in feature[:step]]\n","          #X_train_fs, X_test_fs = X_train_int[:, selected_features], X_test_int[:, selected_features]\n","        for j, s in enumerate(steps):\n","          v = ranking_tree[(n * n_cv) + i][:s]\n","          X_train_fs, X_test_fs = X_train_int[:, v], X_test_int[:, v]\n","          best_model.fit(X_train_fs, y_train_int)\n","      \n","      \n","\n","          #classif_rep = classification_report(y_ts, yp, output_dict=True)\n","          y_pred = best_model.predict(X_test_fs)\n","\n","\n","     \n","\n","\n","          metrics_df_tree  = metrics_df_tree.append(\n","                    {\n","                        \"Iteration\": n,\n","                        \"Fold\": i,\n","                        \"Features\": s,\n","                        \"Recall\": recall_score(y_test_int, y_pred),\n","                        \"Precision\": precision_score(y_test_int,y_pred) ,\n","                      \n","                          'MCC': matthews_corrcoef(y_test_int, y_pred),\n","                      \n","                        \n","                    },\n","                    ignore_index=True,\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pgCIj5jOx-bU","executionInfo":{"status":"ok","timestamp":1645626483946,"user_tz":-60,"elapsed":711810,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"c43d2fd9-dd32-4354-dc98-a8898e5129b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["metrics_df_tree.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"5gw801wf1926","executionInfo":{"status":"ok","timestamp":1645626525778,"user_tz":-60,"elapsed":243,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"27baebd9-797a-4032-fc36-2c3fa68fe93d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-8a8ecb30-abaf-44f4-9349-e4e98910a3a8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Features</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.631579</td>\n","      <td>0.62069</td>\n","      <td>0.371721</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>25.0</td>\n","      <td>0.631579</td>\n","      <td>0.62069</td>\n","      <td>0.371721</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>50.0</td>\n","      <td>0.631579</td>\n","      <td>0.62069</td>\n","      <td>0.371721</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.631579</td>\n","      <td>0.62069</td>\n","      <td>0.371721</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","      <td>0.596491</td>\n","      <td>0.68000</td>\n","      <td>0.418987</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a8ecb30-abaf-44f4-9349-e4e98910a3a8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8a8ecb30-abaf-44f4-9349-e4e98910a3a8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8a8ecb30-abaf-44f4-9349-e4e98910a3a8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Iteration  Fold  Features    Recall  Precision       MCC\n","0        0.0   0.0      10.0  0.631579    0.62069  0.371721\n","1        0.0   0.0      25.0  0.631579    0.62069  0.371721\n","2        0.0   0.0      50.0  0.631579    0.62069  0.371721\n","3        0.0   0.0       NaN  0.631579    0.62069  0.371721\n","4        0.0   1.0      10.0  0.596491    0.68000  0.418987"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["metrics_df_tree['Features'] = metrics_df_tree['Features'].fillna(\"all\")"],"metadata":{"id":"EUA8ekgJ2DG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_df_tree.to_csv('metrics_df_tree.csv', encoding = 'utf-8-sig') "],"metadata":{"id":"B7D6NTcA2HhV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_tree=metrics_df_tree.groupby(['Features']).mean()"],"metadata":{"id":"rPyTzTXZ2MEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_tree"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Wv_DcxGRR6If","executionInfo":{"status":"ok","timestamp":1645631491032,"user_tz":-60,"elapsed":251,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"1278248c-fb3c-4114-f47b-8896e1013ce4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-4cbdbae6-bddb-41b3-893f-8bd352deea00\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","    </tr>\n","    <tr>\n","      <th>Features</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10.0</th>\n","      <td>98.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.537895</td>\n","      <td>0.604456</td>\n","      <td>0.318486</td>\n","    </tr>\n","    <tr>\n","      <th>25.0</th>\n","      <td>99.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.542456</td>\n","      <td>0.600840</td>\n","      <td>0.317098</td>\n","    </tr>\n","    <tr>\n","      <th>50.0</th>\n","      <td>100.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.541053</td>\n","      <td>0.600045</td>\n","      <td>0.314895</td>\n","    </tr>\n","    <tr>\n","      <th>all</th>\n","      <td>101.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.541754</td>\n","      <td>0.599016</td>\n","      <td>0.314494</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4cbdbae6-bddb-41b3-893f-8bd352deea00')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4cbdbae6-bddb-41b3-893f-8bd352deea00 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4cbdbae6-bddb-41b3-893f-8bd352deea00');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["          Unnamed: 0  Iteration  Fold    Recall  Precision       MCC\n","Features                                                            \n","10.0            98.0        4.5   2.0  0.537895   0.604456  0.318486\n","25.0            99.0        4.5   2.0  0.542456   0.600840  0.317098\n","50.0           100.0        4.5   2.0  0.541053   0.600045  0.314895\n","all            101.0        4.5   2.0  0.541754   0.599016  0.314494"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","source":["avg_df_tree.to_csv('avg_df_tree.csv', encoding = 'utf-8-sig') "],"metadata":{"id":"Lx8lop2E2P6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_tree = pd.read_csv('avg_df_tree.csv')\n","avg_df_tree"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"why6hQcdsTQq","executionInfo":{"status":"ok","timestamp":1645627344144,"user_tz":-60,"elapsed":7,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"436ed040-dfb3-499b-aca6-92680462194e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-c34ddb19-b5d8-42c8-a4eb-e05251fa8510\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Features</th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.537895</td>\n","      <td>0.604456</td>\n","      <td>0.318486</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>25.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.542456</td>\n","      <td>0.600840</td>\n","      <td>0.317098</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>50.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.541053</td>\n","      <td>0.600045</td>\n","      <td>0.314895</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>all</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.541754</td>\n","      <td>0.599016</td>\n","      <td>0.314494</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c34ddb19-b5d8-42c8-a4eb-e05251fa8510')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c34ddb19-b5d8-42c8-a4eb-e05251fa8510 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c34ddb19-b5d8-42c8-a4eb-e05251fa8510');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["  Features  Iteration  Fold    Recall  Precision       MCC\n","0     10.0        4.5   2.0  0.537895   0.604456  0.318486\n","1     25.0        4.5   2.0  0.542456   0.600840  0.317098\n","2     50.0        4.5   2.0  0.541053   0.600045  0.314895\n","3      all        4.5   2.0  0.541754   0.599016  0.314494"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["#append confidence intervals\n","\n","cols=['Recall' ,'Precision', 'MCC']\n","\n","for col in cols:\n","  avg_df_tree[col]=metrics_df_tree.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).value)\n","  avg_df_tree['Lower Bound '+ col]=metrics_df_tree.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).lower_bound)\n","  avg_df_tree['Upper Bound '+ col]=metrics_df_tree.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).upper_bound)"],"metadata":{"id":"paJszkfK2Z-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_tree.to_csv('CI_df_tree.csv', encoding = 'utf-8-sig') "],"metadata":{"id":"M3vBpwBk2iSB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CI_df_tree = pd.read_csv('CI_df_tree.csv')\n","CI_df_tree"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"zulCkLxw2moW","executionInfo":{"status":"ok","timestamp":1645631513471,"user_tz":-60,"elapsed":257,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"2f4c270b-697a-4038-a0c8-85066bedaca1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-66839996-6f2f-4b53-a201-a9904a103696\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Features</th>\n","      <th>Unnamed: 0</th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","      <th>Lower Bound Recall</th>\n","      <th>Upper Bound Recall</th>\n","      <th>Lower Bound Precision</th>\n","      <th>Upper Bound Precision</th>\n","      <th>Lower Bound MCC</th>\n","      <th>Upper Bound MCC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.0</td>\n","      <td>98.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.537895</td>\n","      <td>0.604456</td>\n","      <td>0.318486</td>\n","      <td>0.507719</td>\n","      <td>0.572982</td>\n","      <td>0.581373</td>\n","      <td>0.636011</td>\n","      <td>0.297035</td>\n","      <td>0.341559</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>25.0</td>\n","      <td>99.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.542456</td>\n","      <td>0.600840</td>\n","      <td>0.317098</td>\n","      <td>0.514035</td>\n","      <td>0.575789</td>\n","      <td>0.577565</td>\n","      <td>0.632175</td>\n","      <td>0.293949</td>\n","      <td>0.341190</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>50.0</td>\n","      <td>100.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.541053</td>\n","      <td>0.600045</td>\n","      <td>0.314895</td>\n","      <td>0.512281</td>\n","      <td>0.573684</td>\n","      <td>0.575872</td>\n","      <td>0.632156</td>\n","      <td>0.291040</td>\n","      <td>0.339356</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>all</td>\n","      <td>101.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.541754</td>\n","      <td>0.599016</td>\n","      <td>0.314494</td>\n","      <td>0.512982</td>\n","      <td>0.574386</td>\n","      <td>0.575542</td>\n","      <td>0.630481</td>\n","      <td>0.290351</td>\n","      <td>0.339333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66839996-6f2f-4b53-a201-a9904a103696')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-66839996-6f2f-4b53-a201-a9904a103696 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-66839996-6f2f-4b53-a201-a9904a103696');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["  Features  Unnamed: 0  ...  Lower Bound MCC  Upper Bound MCC\n","0     10.0        98.0  ...         0.297035         0.341559\n","1     25.0        99.0  ...         0.293949         0.341190\n","2     50.0       100.0  ...         0.291040         0.339356\n","3      all       101.0  ...         0.290351         0.339333\n","\n","[4 rows x 13 columns]"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["mcc_results.append (('Decision Tree ',\n","                 CI_df_tree[  (CI_df_tree.MCC == CI_df_tree.MCC.values.max())  ] ['MCC'].iloc[0],\n","                 CI_df_tree[  (CI_df_tree.MCC == CI_df_tree.MCC.values.max())  ]  ['Features'].iloc[0] ))"],"metadata":{"id":"QUN1W0azsoWI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LOGISTIC REGRESSION\n"],"metadata":{"id":"dq7b0e_0iN6S"}},{"cell_type":"code","source":["metrics_df_log = pd.DataFrame(columns=[\"Iteration\", \"Fold\", \"Features\" , \"Recall\", \"Precision\", 'MCC'])"],"metadata":{"id":"IOMLFfTkifWq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["log_param = [\n","    {\n","    'model__penalty': ['l2', None],\n","    'model__C': np.logspace(-4, 4, 10),\n","    'model__solver': ['lbfgs'],\n","    'model__class_weight': [None, 'balanced']}\n","]\n","\n","\n","steps = [('preprocessor', preprocessor), ('model', LogisticRegression(random_state=42, max_iter=1000))]\n","log_pipe = Pipeline(steps=steps)\n","\n","scorer = make_scorer(matthews_corrcoef)"],"metadata":{"id":"OWB7iZaGip3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["steps=[10, 25, 50, preprocessor.fit_transform(X_train).shape[1] ] \n","n_ext=10\n","n_cv=5\n","ranking_log = np.empty((n_ext * n_cv, preprocessor.fit_transform(X_train).shape[1]), dtype=int)\n","\n","for n in range(n_ext):\n","    \n","    skf = StratifiedKFold(n_cv, shuffle=True, random_state=n)\n","    \n","    for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n","        X_train_int, X_test_int = X_train.iloc[train_index], X_train.iloc[test_index]\n","        y_train_int, y_test_int = y_train.iloc[train_index], y_train.iloc[test_index]\n","        \n","        tuncv = StratifiedShuffleSplit(\n","                n_splits=n_cv, test_size=0.5, random_state=i\n","            )\n","        log_grid = GridSearchCV(estimator = log_pipe, param_grid = log_param, scoring=scorer, cv=tuncv ,n_jobs=-1, verbose=False)\n","\n","    \n","    \n","        log_grid.fit(X_train_int ,y_train_int) #GridSearchCV over training  fold to find the optimal parameters \n","\n","        best_model = log_grid.best_estimator_.get_params()['model']\n","  \n","        selector_pipe.fit(X_train_int, y_train_int) #feature ranking through SelectKBest\n","\n","        #ordered list of tuples containing the index of the feature and its ranking:\n","        scores=selector.scores_[selector.get_support()]\n","        #feature=list(zip(range(scores.shape[0]), scores))\n","        #feature.sort(key=lambda x:x[1])\n","        ranking_tmp = np.argsort(scores)[::-1]\n","        ranking_log[(n * n_cv) + i] = ranking_tmp\n","\n","    \n","        # rescaling\n","       \n","     \n","        X_train_int = preprocessor.fit_transform(X_train_int)\n","        X_test_int = preprocessor.transform(X_test_int)\n","        \n","\n","        #for step in steps:\n","          #selected_features=[tupla[0] for tupla in feature[:step]]\n","          #X_train_fs, X_test_fs = X_train_int[:, selected_features], X_test_int[:, selected_features]\n","        for j, s in enumerate(steps):\n","          v = ranking_log[(n * n_cv) + i][:s]\n","          X_train_fs, X_test_fs = X_train_int[:, v], X_test_int[:, v]\n","          best_model.fit(X_train_fs, y_train_int)\n","      \n","      \n","\n","          #classif_rep = classification_report(y_ts, yp, output_dict=True)\n","          y_pred = best_model.predict(X_test_fs)\n","\n","\n","     \n","\n","\n","          metrics_df_log  = metrics_df_log.append(\n","                    {\n","                        \"Iteration\": n,\n","                        \"Fold\": i,\n","                        \"Features\": s,\n","                        \"Recall\": recall_score(y_test_int, y_pred),\n","                        \"Precision\": precision_score(y_test_int,y_pred) ,\n","                      \n","                          'MCC': matthews_corrcoef(y_test_int, y_pred),\n","                      \n","                        \n","                    },\n","                    ignore_index=True,\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s91N6pY93NnY","executionInfo":{"status":"ok","timestamp":1645628000880,"user_tz":-60,"elapsed":283501,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"5f0e5432-daa9-4306-8bc9-ad9e6fb36d74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.24661808        nan 0.08937245        nan\n"," 0.27205768        nan 0.21950769        nan 0.27349993        nan\n"," 0.27264353        nan 0.30006712        nan 0.29843153        nan\n"," 0.29321166        nan 0.28012613        nan 0.29572306        nan\n"," 0.27169296        nan 0.28677009        nan 0.27179239        nan\n"," 0.28220145        nan 0.27551933        nan 0.28456284        nan\n"," 0.2772077         nan 0.2833593         nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.25130961        nan 0.09022415        nan\n"," 0.23875279        nan 0.20698282        nan 0.2516019         nan\n"," 0.25700163        nan 0.23627887        nan 0.22989596        nan\n"," 0.23333645        nan 0.21930958        nan 0.23389962        nan\n"," 0.21770948        nan 0.22685517        nan 0.22236798        nan\n"," 0.22540573        nan 0.21964959        nan 0.22305597        nan\n"," 0.21964959        nan 0.22305597        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.28887341        nan 0.07507189        nan\n"," 0.29134388        nan 0.20873085        nan 0.27855333        nan\n"," 0.24998461        nan 0.24555085        nan 0.24022259        nan\n"," 0.24882798        nan 0.23035748        nan 0.23102853        nan\n"," 0.23231674        nan 0.2349109         nan 0.22587428        nan\n"," 0.23382373        nan 0.22077568        nan 0.23720323        nan\n"," 0.22247051        nan 0.23720323        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27505119        nan 0.12774134        nan\n"," 0.27509484        nan 0.29270251        nan 0.28341401        nan\n"," 0.30990127        nan 0.3238698         nan 0.30223647        nan\n"," 0.33536958        nan 0.29529078        nan 0.31092323        nan\n"," 0.30232661        nan 0.30237215        nan 0.30234831        nan\n"," 0.30131401        nan 0.30400131        nan 0.30183633        nan\n"," 0.30064597        nan 0.29843491        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.3303903         nan 0.08308951        nan\n"," 0.34278675        nan 0.29301194        nan 0.34134597        nan\n"," 0.35553707        nan 0.35314228        nan 0.34482003        nan\n"," 0.31377477        nan 0.32837597        nan 0.32217675        nan\n"," 0.31588047        nan 0.310064          nan 0.30712016        nan\n"," 0.29618091        nan 0.31215824        nan 0.29405386        nan\n"," 0.31215824        nan 0.29283276        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27605254        nan 0.09315735        nan\n"," 0.28036454        nan 0.26749025        nan 0.29465969        nan\n"," 0.28203693        nan 0.28350706        nan 0.26870463        nan\n"," 0.28479173        nan 0.27724967        nan 0.275454          nan\n"," 0.27198895        nan 0.25467086        nan 0.26926899        nan\n"," 0.26030324        nan 0.26725791        nan 0.26083671        nan\n"," 0.26595426        nan 0.26083671        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27638822        nan 0.06078688        nan\n"," 0.27456161        nan 0.26219072        nan 0.28550433        nan\n"," 0.28299014        nan 0.31318557        nan 0.28395768        nan\n"," 0.3042877         nan 0.29185239        nan 0.29366751        nan\n"," 0.29359858        nan 0.28884056        nan 0.29553526        nan\n"," 0.28191467        nan 0.29217218        nan 0.27862699        nan\n"," 0.29010194        nan 0.27810637        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.24818368        nan 0.08823305        nan\n"," 0.24347169        nan 0.2219949         nan 0.2665775         nan\n"," 0.29207527        nan 0.26336083        nan 0.263823          nan\n"," 0.25483757        nan 0.23487565        nan 0.23469695        nan\n"," 0.2335244         nan 0.23052355        nan 0.22705452        nan\n"," 0.22780564        nan 0.22933668        nan 0.22439198        nan\n"," 0.22798285        nan 0.22561537        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.2412817         nan 0.08667693        nan\n"," 0.25108186        nan 0.20834737        nan 0.25100345        nan\n"," 0.21412089        nan 0.22032109        nan 0.19825674        nan\n"," 0.20895361        nan 0.21486062        nan 0.20454784        nan\n"," 0.20949986        nan 0.19233241        nan 0.21636952        nan\n"," 0.1980842         nan 0.21041282        nan 0.19401749        nan\n"," 0.21212785        nan 0.19230963        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27821296        nan 0.0785208         nan\n"," 0.29159414        nan 0.25343487        nan 0.29637262        nan\n"," 0.28472705        nan 0.26867889        nan 0.26900144        nan\n"," 0.257124          nan 0.25300209        nan 0.25276176        nan\n"," 0.23623088        nan 0.2298922         nan 0.23687968        nan\n"," 0.22767662        nan 0.23683527        nan 0.22767662        nan\n"," 0.23683527        nan 0.22889516        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.28856933        nan 0.08873685        nan\n"," 0.30176724        nan 0.27924222        nan 0.3056984         nan\n"," 0.32942876        nan 0.32575778        nan 0.31963317        nan\n"," 0.30051251        nan 0.2933049         nan 0.28931847        nan\n"," 0.29111083        nan 0.28137504        nan 0.28438887        nan\n"," 0.28897721        nan 0.28877777        nan 0.28814968        nan\n"," 0.28409637        nan 0.28646614        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27108238        nan 0.04146192        nan\n"," 0.27045692        nan 0.22800731        nan 0.29384724        nan\n"," 0.27274939        nan 0.28665054        nan 0.27119133        nan\n"," 0.29424508        nan 0.26500898        nan 0.29984508        nan\n"," 0.25259468        nan 0.29661089        nan 0.25994049        nan\n"," 0.29029484        nan 0.25058288        nan 0.27720998        nan\n"," 0.24723206        nan 0.27382024        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.31765014        nan 0.10725559        nan\n"," 0.31606064        nan 0.2439912         nan 0.28995796        nan\n"," 0.28195306        nan 0.25712592        nan 0.25324209        nan\n"," 0.2631076         nan 0.24328163        nan 0.25289762        nan\n"," 0.20881159        nan 0.24280213        nan 0.21003694        nan\n"," 0.2397464         nan 0.21344008        nan 0.23844901        nan\n"," 0.21344008        nan 0.23844901        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.26065495        nan 0.06170406        nan\n"," 0.26190129        nan 0.20699067        nan 0.28412673        nan\n"," 0.26908579        nan 0.28327547        nan 0.2782328         nan\n"," 0.27364721        nan 0.26803471        nan 0.28361137        nan\n"," 0.259787          nan 0.28015299        nan 0.2538637         nan\n"," 0.27960097        nan 0.25725765        nan 0.28010478        nan\n"," 0.25895148        nan 0.28010478        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.28054399        nan 0.10153939        nan\n"," 0.27404436        nan 0.26762494        nan 0.27842604        nan\n"," 0.25598455        nan 0.2839289         nan 0.25796816        nan\n"," 0.28722846        nan 0.22339599        nan 0.26966914        nan\n"," 0.23444635        nan 0.26178152        nan 0.23859982        nan\n"," 0.25633742        nan 0.23729422        nan 0.25974553        nan\n"," 0.2389576         nan 0.26090444        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.29570105        nan 0.08740404        nan\n"," 0.30115339        nan 0.25535729        nan 0.30882945        nan\n"," 0.3140013         nan 0.32410213        nan 0.29579953        nan\n"," 0.30501338        nan 0.28204082        nan 0.2772747         nan\n"," 0.26505099        nan 0.28303431        nan 0.25717304        nan\n"," 0.26701062        nan 0.25448434        nan 0.27036816        nan\n"," 0.25616928        nan 0.27036816        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.25977719        nan 0.05702327        nan\n"," 0.27854297        nan 0.23073667        nan 0.26785814        nan\n"," 0.2838348         nan 0.30222632        nan 0.29818431        nan\n"," 0.2857364         nan 0.28040048        nan 0.28450909        nan\n"," 0.27548267        nan 0.27722428        nan 0.27018703        nan\n"," 0.28155612        nan 0.26954893        nan 0.27234294        nan\n"," 0.26749783        nan 0.27532328        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.24601816        nan 0.0445614         nan\n"," 0.25736624        nan 0.200348          nan 0.2554542         nan\n"," 0.22571067        nan 0.22783661        nan 0.20624106        nan\n"," 0.24744724        nan 0.20691079        nan 0.24299882        nan\n"," 0.21674071        nan 0.23544638        nan 0.21374629        nan\n"," 0.23138723        nan 0.21712813        nan 0.23138723        nan\n"," 0.21712813        nan 0.23138723        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27306071        nan 0.0725512         nan\n"," 0.29416461        nan 0.29539125        nan 0.3146935         nan\n"," 0.33071675        nan 0.3118578         nan 0.2855825         nan\n"," 0.3109164         nan 0.2744761         nan 0.29811834        nan\n"," 0.27181592        nan 0.3005658         nan 0.26348658        nan\n"," 0.29627182        nan 0.26179963        nan 0.28999041        nan\n"," 0.26179963        nan 0.28999041        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.29501072        nan 0.04593322        nan\n"," 0.30515179        nan 0.24299694        nan 0.31602546        nan\n"," 0.26966553        nan 0.28389907        nan 0.26341079        nan\n"," 0.29020773        nan 0.25022304        nan 0.27934777        nan\n"," 0.25015158        nan 0.27482331        nan 0.24713871        nan\n"," 0.27425895        nan 0.24880848        nan 0.2714628         nan\n"," 0.24880848        nan 0.27313465        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.28350171        nan 0.03964621        nan\n"," 0.29801639        nan 0.2457726         nan 0.3040156         nan\n"," 0.2803312         nan 0.29940928        nan 0.26491585        nan\n"," 0.27005475        nan 0.24893727        nan 0.24251505        nan\n"," 0.26071041        nan 0.24972399        nan 0.25728966        nan\n"," 0.24857347        nan 0.25553145        nan 0.24869419        nan\n"," 0.25553145        nan 0.24579904        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27111451        nan 0.04021886        nan\n"," 0.28820832        nan 0.28060323        nan 0.3069323         nan\n"," 0.32656464        nan 0.30063716        nan 0.30936569        nan\n"," 0.30293251        nan 0.28201775        nan 0.29002241        nan\n"," 0.28204205        nan 0.29364448        nan 0.27801623        nan\n"," 0.29693319        nan 0.27449213        nan 0.29617315        nan\n"," 0.27618276        nan 0.29617315        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.31435952        nan 0.09540753        nan\n"," 0.3159698         nan 0.2566196         nan 0.30839966        nan\n"," 0.27488939        nan 0.30693852        nan 0.29309869        nan\n"," 0.31024271        nan 0.27646144        nan 0.28989789        nan\n"," 0.26934835        nan 0.29020738        nan 0.26620645        nan\n"," 0.28273583        nan 0.26451643        nan 0.27814903        nan\n"," 0.26282098        nan 0.27646065        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27627859        nan 0.05765917        nan\n"," 0.29318598        nan 0.24698676        nan 0.27989008        nan\n"," 0.27837651        nan 0.27306631        nan 0.26330959        nan\n"," 0.27368295        nan 0.24921596        nan 0.26028295        nan\n"," 0.25132657        nan 0.24916114        nan 0.24148402        nan\n"," 0.25735276        nan 0.24055198        nan 0.25262121        nan\n"," 0.24055198        nan 0.25091808        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.31185626        nan 0.13151454        nan\n"," 0.29666205        nan 0.26395885        nan 0.30601497        nan\n"," 0.26727891        nan 0.279597          nan 0.26194308        nan\n"," 0.25668728        nan 0.2303748         nan 0.25269268        nan\n"," 0.23187751        nan 0.25307248        nan 0.2295755         nan\n"," 0.26143027        nan 0.23285644        nan 0.2602091         nan\n"," 0.23153996        nan 0.25893869        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27119092        nan 0.05920905        nan\n"," 0.28083363        nan 0.24517393        nan 0.2887261         nan\n"," 0.28619884        nan 0.26866554        nan 0.26888247        nan\n"," 0.2638636         nan 0.24103852        nan 0.23432771        nan\n"," 0.23654915        nan 0.2347751         nan 0.2334063         nan\n"," 0.2345848         nan 0.24224974        nan 0.23288276        nan\n"," 0.23926975        nan 0.23117506        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.30643293        nan 0.09673953        nan\n"," 0.29852289        nan 0.26265693        nan 0.29221486        nan\n"," 0.26177526        nan 0.29803829        nan 0.26241343        nan\n"," 0.28386323        nan 0.25759104        nan 0.26162799        nan\n"," 0.24749024        nan 0.26699685        nan 0.23349374        nan\n"," 0.25773238        nan 0.23518297        nan 0.25654077        nan\n"," 0.23518297        nan 0.25483815        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.25917668        nan 0.0760427         nan\n"," 0.25844633        nan 0.2374217         nan 0.2746642         nan\n"," 0.28202615        nan 0.28531923        nan 0.25950784        nan\n"," 0.2590444         nan 0.25393603        nan 0.25038305        nan\n"," 0.25886762        nan 0.24888762        nan 0.24767981        nan\n"," 0.24454755        nan 0.24132629        nan 0.24385188        nan\n"," 0.24132629        nan 0.24385188        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.28574925        nan 0.08548756        nan\n"," 0.28198151        nan 0.24887947        nan 0.27885883        nan\n"," 0.28888568        nan 0.29460559        nan 0.26354323        nan\n"," 0.28273617        nan 0.24709578        nan 0.26247072        nan\n"," 0.25027007        nan 0.27156758        nan 0.25145976        nan\n"," 0.26647628        nan 0.25017429        nan 0.26530909        nan\n"," 0.25017429        nan 0.26530909        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.26554868        nan 0.10422469        nan\n"," 0.26551766        nan 0.25872385        nan 0.27220977        nan\n"," 0.27281326        nan 0.25996584        nan 0.26297172        nan\n"," 0.26819139        nan 0.25808376        nan 0.26991533        nan\n"," 0.26015813        nan 0.2715063         nan 0.25749036        nan\n"," 0.26843775        nan 0.25749036        nan 0.26968208        nan\n"," 0.25749036        nan 0.26968208        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.31471357        nan 0.11636049        nan\n"," 0.30987896        nan 0.278069          nan 0.31047865        nan\n"," 0.28982792        nan 0.30431214        nan 0.28904377        nan\n"," 0.28096218        nan 0.27193784        nan 0.26061204        nan\n"," 0.25768096        nan 0.26351995        nan 0.25097776        nan\n"," 0.26010441        nan 0.24763051        nan 0.25666443        nan\n"," 0.24462261        nan 0.2549688         nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.25679307        nan 0.11443883        nan\n"," 0.25257104        nan 0.27616096        nan 0.25160847        nan\n"," 0.25507646        nan 0.26471852        nan 0.21420186        nan\n"," 0.24372001        nan 0.22758885        nan 0.23669283        nan\n"," 0.22671481        nan 0.22456059        nan 0.22566467        nan\n"," 0.22282685        nan 0.22254809        nan 0.22001255        nan\n"," 0.21963966        nan 0.21616652        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.26123662        nan 0.13781778        nan\n"," 0.27010163        nan 0.24470727        nan 0.30937004        nan\n"," 0.30770789        nan 0.33338766        nan 0.30793841        nan\n"," 0.33206287        nan 0.29438621        nan 0.31186644        nan\n"," 0.2843162         nan 0.29209298        nan 0.27667935        nan\n"," 0.29551636        nan 0.27369097        nan 0.29722674        nan\n"," 0.2720504         nan 0.29722674        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.28270572        nan 0.05746008        nan\n"," 0.28594953        nan 0.27032143        nan 0.33038842        nan\n"," 0.31003107        nan 0.33064537        nan 0.31557979        nan\n"," 0.3209066         nan 0.30216123        nan 0.31088533        nan\n"," 0.30071282        nan 0.3006525         nan 0.29877178        nan\n"," 0.29776795        nan 0.29712644        nan 0.29652362        nan\n"," 0.29839915        nan 0.29652362        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.21341786        nan 0.09074019        nan\n"," 0.22902948        nan 0.20582731        nan 0.22677155        nan\n"," 0.23152898        nan 0.22853112        nan 0.24251931        nan\n"," 0.24197059        nan 0.24420081        nan 0.22965086        nan\n"," 0.24511398        nan 0.22736802        nan 0.23781361        nan\n"," 0.22514462        nan 0.23611965        nan 0.22514462        nan\n"," 0.23471649        nan 0.22276192        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.24725461        nan 0.01444114        nan\n"," 0.26257443        nan 0.18617094        nan 0.24619705        nan\n"," 0.19571645        nan 0.23326725        nan 0.21427781        nan\n"," 0.20348763        nan 0.21252932        nan 0.2002705         nan\n"," 0.19753351        nan 0.20367699        nan 0.19903259        nan\n"," 0.19159223        nan 0.19903259        nan 0.18861636        nan\n"," 0.19734268        nan 0.18691517        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.28375484        nan 0.08656489        nan\n"," 0.29040855        nan 0.27559358        nan 0.30238817        nan\n"," 0.31539636        nan 0.30020111        nan 0.31773892        nan\n"," 0.27718332        nan 0.30954029        nan 0.26840347        nan\n"," 0.31508044        nan 0.26942993        nan 0.31536984        nan\n"," 0.27405697        nan 0.31370293        nan 0.27353966        nan\n"," 0.31370293        nan 0.27353966        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.29512755        nan 0.07043162        nan\n"," 0.28949309        nan 0.22198734        nan 0.30285016        nan\n"," 0.27769685        nan 0.28458421        nan 0.25595156        nan\n"," 0.28037283        nan 0.25874436        nan 0.25928255        nan\n"," 0.24411385        nan 0.25071977        nan 0.24051166        nan\n"," 0.23450541        nan 0.23846704        nan 0.22701431        nan\n"," 0.23710759        nan 0.23087021        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.28458201        nan 0.06662646        nan\n"," 0.28482679        nan 0.27021942        nan 0.29129216        nan\n"," 0.29749623        nan 0.29403333        nan 0.28167058        nan\n"," 0.29211979        nan 0.26740075        nan 0.27615015        nan\n"," 0.2478356         nan 0.27164377        nan 0.2425012         nan\n"," 0.26536049        nan 0.24244716        nan 0.26823494        nan\n"," 0.24413398        nan 0.26823494        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27708511        nan 0.09322175        nan\n"," 0.2859544         nan 0.26216241        nan 0.29270736        nan\n"," 0.28579757        nan 0.30965071        nan 0.2716977         nan\n"," 0.30468399        nan 0.26299635        nan 0.28986733        nan\n"," 0.25165028        nan 0.27866981        nan 0.24140848        nan\n"," 0.27552994        nan 0.24271595        nan 0.27437627        nan\n"," 0.2427264         nan 0.27437627        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.30132143        nan 0.10393646        nan\n"," 0.31973841        nan 0.27946556        nan 0.31535399        nan\n"," 0.33534123        nan 0.32052349        nan 0.32057521        nan\n"," 0.34311122        nan 0.31728113        nan 0.31754445        nan\n"," 0.30544143        nan 0.32127079        nan 0.3031893         nan\n"," 0.30633334        nan 0.29895647        nan 0.30338955        nan\n"," 0.3006214         nan 0.30291077        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27400195        nan 0.08484538        nan\n"," 0.27874602        nan 0.24249025        nan 0.28843963        nan\n"," 0.27694702        nan 0.30468962        nan 0.27862674        nan\n"," 0.26885848        nan 0.26824475        nan 0.26693162        nan\n"," 0.25423999        nan 0.25262463        nan 0.25283494        nan\n"," 0.25615484        nan 0.25070203        nan 0.24813122        nan\n"," 0.25029631        nan 0.24813122        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.20767492        nan 0.03508609        nan\n"," 0.21274894        nan 0.24890295        nan 0.2199348         nan\n"," 0.22842265        nan 0.24581734        nan 0.20342115        nan\n"," 0.21961886        nan 0.18476504        nan 0.20123757        nan\n"," 0.20118558        nan 0.19667998        nan 0.19641422        nan\n"," 0.19833369        nan 0.1916909         nan 0.19494741        nan\n"," 0.1916909         nan 0.19492523        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.30187353        nan 0.09423681        nan\n"," 0.3080006         nan 0.26370842        nan 0.29882398        nan\n"," 0.26557477        nan 0.29017477        nan 0.27688505        nan\n"," 0.28475409        nan 0.26024549        nan 0.28249652        nan\n"," 0.24855043        nan 0.26016561        nan 0.24085553        nan\n"," 0.26035836        nan 0.23994287        nan 0.25865713        nan\n"," 0.23656419        nan 0.25865635        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.25799483        nan 0.07888716        nan\n"," 0.26528621        nan 0.25050139        nan 0.28596293        nan\n"," 0.25150939        nan 0.29807101        nan 0.25318483        nan\n"," 0.27747124        nan 0.24383143        nan 0.25162117        nan\n"," 0.24193219        nan 0.24802837        nan 0.25148747        nan\n"," 0.23298774        nan 0.25447569        nan 0.23298321        nan\n"," 0.25582253        nan 0.23420466        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.28056673        nan 0.13615009        nan\n"," 0.28190447        nan 0.27891613        nan 0.29209088        nan\n"," 0.28655687        nan 0.2758462         nan 0.2884782         nan\n"," 0.28958938        nan 0.27590906        nan 0.28126151        nan\n"," 0.27309463        nan 0.2769121         nan 0.27881649        nan\n"," 0.26607951        nan 0.27509599        nan 0.26777591        nan\n"," 0.27509599        nan 0.26777591        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.29164112        nan 0.07916831        nan\n"," 0.30234293        nan 0.24499819        nan 0.30772803        nan\n"," 0.28999208        nan 0.34543228        nan 0.30419103        nan\n"," 0.33589583        nan 0.29114747        nan 0.31460529        nan\n"," 0.30517985        nan 0.31266263        nan 0.30198236        nan\n"," 0.31981171        nan 0.29824268        nan 0.31747669        nan\n"," 0.29948421        nan 0.31994792        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.27242884        nan 0.08245385        nan\n"," 0.28107948        nan 0.25809878        nan 0.26407042        nan\n"," 0.24950641        nan 0.27802282        nan 0.22596341        nan\n"," 0.26311126        nan 0.21677264        nan 0.23766878        nan\n"," 0.21007709        nan 0.22823275        nan 0.20229123        nan\n"," 0.22751275        nan 0.20100035        nan 0.22581382        nan\n"," 0.20066098        nan 0.22413695        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.32232072        nan 0.08690062        nan\n"," 0.33844836        nan 0.25701002        nan 0.35957764        nan\n"," 0.30387083        nan 0.32757503        nan 0.29152014        nan\n"," 0.27909722        nan 0.26656701        nan 0.26519849        nan\n"," 0.25229853        nan 0.26738647        nan 0.25000143        nan\n"," 0.25405315        nan 0.24687107        nan 0.25333516        nan\n"," 0.24550376        nan 0.25333516        nan]\n","  category=UserWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n","100 fits failed out of a total of 200.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","100 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\", line 266, in fit\n","    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n","    % (all_penalties, penalty)\n","ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.                nan 0.29007214        nan 0.05039424        nan\n"," 0.27948218        nan 0.2665397         nan 0.29527563        nan\n"," 0.28932698        nan 0.28683477        nan 0.28140697        nan\n"," 0.29890722        nan 0.25950198        nan 0.29593398        nan\n"," 0.26572772        nan 0.2714103         nan 0.26202521        nan\n"," 0.27069943        nan 0.26339737        nan 0.26539003        nan\n"," 0.26339737        nan 0.26654894        nan]\n","  category=UserWarning,\n"]}]},{"cell_type":"code","source":["metrics_df_log.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"1L023iFT4rxa","executionInfo":{"status":"ok","timestamp":1645628013665,"user_tz":-60,"elapsed":233,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"94b58f7a-e9e8-46aa-9b2e-37e082375fe2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-178c23fd-571b-4c78-91d0-050c6fbbf191\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Features</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.649123</td>\n","      <td>0.616667</td>\n","      <td>0.375638</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>25.0</td>\n","      <td>0.614035</td>\n","      <td>0.593220</td>\n","      <td>0.329916</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>50.0</td>\n","      <td>0.578947</td>\n","      <td>0.600000</td>\n","      <td>0.322125</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>67.0</td>\n","      <td>0.596491</td>\n","      <td>0.576271</td>\n","      <td>0.300764</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","      <td>0.403509</td>\n","      <td>0.676471</td>\n","      <td>0.314847</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-178c23fd-571b-4c78-91d0-050c6fbbf191')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-178c23fd-571b-4c78-91d0-050c6fbbf191 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-178c23fd-571b-4c78-91d0-050c6fbbf191');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Iteration  Fold  Features    Recall  Precision       MCC\n","0        0.0   0.0      10.0  0.649123   0.616667  0.375638\n","1        0.0   0.0      25.0  0.614035   0.593220  0.329916\n","2        0.0   0.0      50.0  0.578947   0.600000  0.322125\n","3        0.0   0.0      67.0  0.596491   0.576271  0.300764\n","4        0.0   1.0      10.0  0.403509   0.676471  0.314847"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["metrics_df_log.to_csv('metrics_df_log.csv', encoding = 'utf-8-sig') "],"metadata":{"id":"fyswYQdm41Je"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_df_log=pd.read_csv('metrics_df_log.csv')"],"metadata":{"id":"m6x83xdNGr2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_log=metrics_df_log.groupby(['Features']).mean()"],"metadata":{"id":"6WmfSHVx47PK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_log.to_csv('avg_df_log.csv', encoding = 'utf-8-sig') "],"metadata":{"id":"TKgo2L1b4_uy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_log = pd.read_csv('avg_df_log.csv')"],"metadata":{"id":"Uh4LI-LX5dI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_log"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"FISQH1tj5BC_","executionInfo":{"status":"ok","timestamp":1645628055216,"user_tz":-60,"elapsed":211,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"82cc2f16-46b0-4d29-b24f-728e2913a2ad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-480204fc-1466-48a2-a343-7385c040dc35\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","    </tr>\n","    <tr>\n","      <th>Features</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10.0</th>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.568070</td>\n","      <td>0.582732</td>\n","      <td>0.290096</td>\n","    </tr>\n","    <tr>\n","      <th>25.0</th>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.571930</td>\n","      <td>0.571216</td>\n","      <td>0.280747</td>\n","    </tr>\n","    <tr>\n","      <th>50.0</th>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.574737</td>\n","      <td>0.576907</td>\n","      <td>0.288158</td>\n","    </tr>\n","    <tr>\n","      <th>67.0</th>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.578947</td>\n","      <td>0.575601</td>\n","      <td>0.288023</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-480204fc-1466-48a2-a343-7385c040dc35')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-480204fc-1466-48a2-a343-7385c040dc35 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-480204fc-1466-48a2-a343-7385c040dc35');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["          Iteration  Fold    Recall  Precision       MCC\n","Features                                                \n","10.0            4.5   2.0  0.568070   0.582732  0.290096\n","25.0            4.5   2.0  0.571930   0.571216  0.280747\n","50.0            4.5   2.0  0.574737   0.576907  0.288158\n","67.0            4.5   2.0  0.578947   0.575601  0.288023"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["#append confidence intervals\n","cols=['Recall' ,'Precision', 'MCC']\n","\n","for col in cols:\n","  avg_df_log[col]=metrics_df_log.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).value)\n","  avg_df_log['Lower Bound '+ col]=metrics_df_log.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).lower_bound)\n","  avg_df_log['Upper Bound '+ col]=metrics_df_log.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).upper_bound)\n","\n"],"metadata":{"id":"0xdYnG8z5IMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_log.to_csv('CI_df_log.csv', encoding = 'utf-8-sig')"],"metadata":{"id":"cjSPud2R5mKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CI_df_log = pd.read_csv('CI_df_log.csv')\n","CI_df_log"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"YvMwd8kK5sAN","executionInfo":{"status":"ok","timestamp":1645628088892,"user_tz":-60,"elapsed":229,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"670f4253-0053-41f5-a4a7-ce0b87213d6b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-8cbf3283-9852-4dae-a40e-84dfa242e4e0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Features</th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","      <th>Lower Bound Recall</th>\n","      <th>Upper Bound Recall</th>\n","      <th>Lower Bound Precision</th>\n","      <th>Upper Bound Precision</th>\n","      <th>Lower Bound MCC</th>\n","      <th>Upper Bound MCC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.568070</td>\n","      <td>0.582732</td>\n","      <td>0.290096</td>\n","      <td>0.535088</td>\n","      <td>0.603509</td>\n","      <td>0.565351</td>\n","      <td>0.599195</td>\n","      <td>0.269789</td>\n","      <td>0.310596</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>25.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.571930</td>\n","      <td>0.571216</td>\n","      <td>0.280747</td>\n","      <td>0.539649</td>\n","      <td>0.605263</td>\n","      <td>0.556214</td>\n","      <td>0.585607</td>\n","      <td>0.258358</td>\n","      <td>0.302657</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>50.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.574737</td>\n","      <td>0.576907</td>\n","      <td>0.288158</td>\n","      <td>0.545614</td>\n","      <td>0.605263</td>\n","      <td>0.560984</td>\n","      <td>0.592228</td>\n","      <td>0.266219</td>\n","      <td>0.309070</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>67.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.578947</td>\n","      <td>0.575601</td>\n","      <td>0.288023</td>\n","      <td>0.550877</td>\n","      <td>0.607368</td>\n","      <td>0.559341</td>\n","      <td>0.590883</td>\n","      <td>0.268053</td>\n","      <td>0.307954</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8cbf3283-9852-4dae-a40e-84dfa242e4e0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8cbf3283-9852-4dae-a40e-84dfa242e4e0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8cbf3283-9852-4dae-a40e-84dfa242e4e0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Features  Iteration  ...  Lower Bound MCC  Upper Bound MCC\n","0      10.0        4.5  ...         0.269789         0.310596\n","1      25.0        4.5  ...         0.258358         0.302657\n","2      50.0        4.5  ...         0.266219         0.309070\n","3      67.0        4.5  ...         0.268053         0.307954\n","\n","[4 rows x 12 columns]"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["mcc_results.append (('Logistic Regression ',\n","                 CI_df_log[  (CI_df_log.MCC == CI_df_log.MCC.values.max())  ] ['MCC'].iloc[0],\n","                 CI_df_log[  (CI_df_log.MCC == CI_df_log.MCC.values.max())  ]  ['Features'].iloc[0] ))"],"metadata":{"id":"CANkGbSPrR8t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["KNN"],"metadata":{"id":"xdwY9hCDpNNo"}},{"cell_type":"code","source":["metrics_df_knn = pd.DataFrame(columns=[\"Iteration\", \"Fold\", \"Features\" , \"Recall\", \"Precision\", 'MCC'])"],"metadata":{"id":"w-XZfK_P6AhF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["knn_param= {'model__n_neighbors': np.arange(1, 25)}\n","\n","steps = [('preprocessor', preprocessor), ('model', KNeighborsClassifier())]\n","knn_pipe = Pipeline(steps=steps)\n","\n","scorer = make_scorer(matthews_corrcoef)"],"metadata":{"id":"v0OunXky6DZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["steps=[10, 25, 50, preprocessor.fit_transform(X_train).shape[1] ] \n","n_ext=10\n","n_cv=5\n","ranking_knn = np.empty((n_ext * n_cv, preprocessor.fit_transform(X_train).shape[1]), dtype=int)\n","\n","for n in range(n_ext):\n","    \n","    skf = StratifiedKFold(n_cv, shuffle=True, random_state=n)\n","    \n","    for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n","        X_train_int, X_test_int = X_train.iloc[train_index], X_train.iloc[test_index]\n","        y_train_int, y_test_int = y_train.iloc[train_index], y_train.iloc[test_index]\n","        \n","        tuncv = StratifiedShuffleSplit(\n","                n_splits=n_cv, test_size=0.5, random_state=i\n","            )\n","        knn_grid = GridSearchCV(estimator = knn_pipe, param_grid = knn_param, scoring=scorer, cv=tuncv ,n_jobs=-1, verbose=False)\n","\n","    \n","    \n","        knn_grid.fit(X_train_int ,y_train_int) #GridSearchCV over training  fold to find the optimal parameters \n","\n","        best_model = knn_grid.best_estimator_.get_params()['model']\n","  \n","        selector_pipe.fit(X_train_int, y_train_int) #feature ranking through SelectKBest\n","\n","        #ordered list of tuples containing the index of the feature and its ranking:\n","        scores=selector.scores_[selector.get_support()]\n","        #feature=list(zip(range(scores.shape[0]), scores))\n","        #feature.sort(key=lambda x:x[1])\n","        ranking_tmp = np.argsort(scores)[::-1]\n","        ranking_knn[(n * n_cv) + i] = ranking_tmp\n","\n","    \n","        # rescaling\n","       \n","     \n","        X_train_int = preprocessor.fit_transform(X_train_int)\n","        X_test_int = preprocessor.transform(X_test_int)\n","        \n","\n","        #for step in steps:\n","          #selected_features=[tupla[0] for tupla in feature[:step]]\n","          #X_train_fs, X_test_fs = X_train_int[:, selected_features], X_test_int[:, selected_features]\n","        for j, s in enumerate(steps):\n","          v = ranking_knn[(n * n_cv) + i][:s]\n","          X_train_fs, X_test_fs = X_train_int[:, v], X_test_int[:, v]\n","          best_model.fit(X_train_fs, y_train_int)\n","      \n","      \n","\n","          #classif_rep = classification_report(y_ts, yp, output_dict=True)\n","          y_pred = best_model.predict(X_test_fs)\n","\n","\n","     \n","\n","\n","          metrics_df_knn  = metrics_df_knn.append(\n","                    {\n","                        \"Iteration\": n,\n","                        \"Fold\": i,\n","                        \"Features\": s,\n","                        \"Recall\": recall_score(y_test_int, y_pred),\n","                        \"Precision\": precision_score(y_test_int,y_pred) ,\n","                      \n","                          'MCC': matthews_corrcoef(y_test_int, y_pred),\n","                      \n","                        \n","                    },\n","                    ignore_index=True,\n","                )"],"metadata":{"id":"mfEXllRH6Gw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_df_knn.to_csv('metrics_df_knn.csv', encoding = 'utf-8-sig') "],"metadata":{"id":"3RgPOr3A7MoX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_df_knn=pd.read_csv('metrics_df_knn.csv')"],"metadata":{"id":"Ji9PLA-Yh1mn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_knn=metrics_df_knn.groupby(['Features']).mean()"],"metadata":{"id":"YV5dJBqk7Ph-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_knn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"aNcnegRm7S8A","executionInfo":{"status":"ok","timestamp":1645628475252,"user_tz":-60,"elapsed":10,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"760baecf-84b7-41d5-d4d2-1408809c5004"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-da72742a-19b8-4e23-b3aa-953fdd0a5983\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","    </tr>\n","    <tr>\n","      <th>Features</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10.0</th>\n","      <td>98.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.454386</td>\n","      <td>0.581748</td>\n","      <td>0.246340</td>\n","    </tr>\n","    <tr>\n","      <th>25.0</th>\n","      <td>99.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.440351</td>\n","      <td>0.575396</td>\n","      <td>0.234117</td>\n","    </tr>\n","    <tr>\n","      <th>50.0</th>\n","      <td>100.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.402807</td>\n","      <td>0.594390</td>\n","      <td>0.238144</td>\n","    </tr>\n","    <tr>\n","      <th>67.0</th>\n","      <td>101.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.401404</td>\n","      <td>0.582244</td>\n","      <td>0.225311</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da72742a-19b8-4e23-b3aa-953fdd0a5983')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-da72742a-19b8-4e23-b3aa-953fdd0a5983 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-da72742a-19b8-4e23-b3aa-953fdd0a5983');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["          Unnamed: 0  Iteration  Fold    Recall  Precision       MCC\n","Features                                                            \n","10.0            98.0        4.5   2.0  0.454386   0.581748  0.246340\n","25.0            99.0        4.5   2.0  0.440351   0.575396  0.234117\n","50.0           100.0        4.5   2.0  0.402807   0.594390  0.238144\n","67.0           101.0        4.5   2.0  0.401404   0.582244  0.225311"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["avg_df_knn.to_csv('avg_df_knn.csv', encoding = 'utf-8-sig')"],"metadata":{"id":"OmRXqyXq7YUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#append confidence intervals\n","cols=['Recall' ,'Precision', 'MCC']\n","\n","for col in cols:\n","  avg_df_knn[col]=metrics_df_knn.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).value)\n","  avg_df_knn['Lower Bound '+ col]=metrics_df_knn.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).lower_bound)\n","  avg_df_knn['Upper Bound '+ col]=metrics_df_knn.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).upper_bound)"],"metadata":{"id":"VUM1myKj7cD0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_knn.to_csv('CI_df_knn.csv', encoding = 'utf-8-sig') "],"metadata":{"id":"CE8raxh37lUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CI_df_knn = pd.read_csv('CI_df_knn.csv')\n","CI_df_knn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"RR9X6SrZ7mKE","executionInfo":{"status":"ok","timestamp":1645628494497,"user_tz":-60,"elapsed":11,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"e7b0a470-3f4d-4700-9580-b3e048d5d1c0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-002918e2-6313-4ba3-a1ea-e88d6021103a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Features</th>\n","      <th>Unnamed: 0</th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","      <th>Lower Bound Recall</th>\n","      <th>Upper Bound Recall</th>\n","      <th>Lower Bound Precision</th>\n","      <th>Upper Bound Precision</th>\n","      <th>Lower Bound MCC</th>\n","      <th>Upper Bound MCC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.0</td>\n","      <td>98.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.454386</td>\n","      <td>0.581748</td>\n","      <td>0.246340</td>\n","      <td>0.432982</td>\n","      <td>0.476500</td>\n","      <td>0.563768</td>\n","      <td>0.599851</td>\n","      <td>0.222152</td>\n","      <td>0.270300</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>25.0</td>\n","      <td>99.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.440351</td>\n","      <td>0.575396</td>\n","      <td>0.234117</td>\n","      <td>0.417895</td>\n","      <td>0.462807</td>\n","      <td>0.556111</td>\n","      <td>0.593834</td>\n","      <td>0.209563</td>\n","      <td>0.257734</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>50.0</td>\n","      <td>100.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.402807</td>\n","      <td>0.594390</td>\n","      <td>0.238144</td>\n","      <td>0.383860</td>\n","      <td>0.422456</td>\n","      <td>0.573152</td>\n","      <td>0.614885</td>\n","      <td>0.213666</td>\n","      <td>0.261961</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>67.0</td>\n","      <td>101.0</td>\n","      <td>4.5</td>\n","      <td>2.0</td>\n","      <td>0.401404</td>\n","      <td>0.582244</td>\n","      <td>0.225311</td>\n","      <td>0.380702</td>\n","      <td>0.422456</td>\n","      <td>0.567248</td>\n","      <td>0.596588</td>\n","      <td>0.208408</td>\n","      <td>0.241905</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-002918e2-6313-4ba3-a1ea-e88d6021103a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-002918e2-6313-4ba3-a1ea-e88d6021103a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-002918e2-6313-4ba3-a1ea-e88d6021103a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Features  Unnamed: 0  ...  Lower Bound MCC  Upper Bound MCC\n","0      10.0        98.0  ...         0.222152         0.270300\n","1      25.0        99.0  ...         0.209563         0.257734\n","2      50.0       100.0  ...         0.213666         0.261961\n","3      67.0       101.0  ...         0.208408         0.241905\n","\n","[4 rows x 13 columns]"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["mcc_results.append (('KNN',\n","                 CI_df_knn[  (CI_df_knn.MCC == CI_df_knn.MCC.values.max())  ] ['MCC'].iloc[0],\n","                 CI_df_knn[  (CI_df_knn.MCC == CI_df_knn.MCC.values.max())  ]  ['Features'].iloc[0] ))"],"metadata":{"id":"jUOSIx_Yrh14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["RANDOM FOREST \n"],"metadata":{"id":"yAdQYYohsP97"}},{"cell_type":"code","source":["metrics_df_rf = pd.DataFrame(columns=[\"Iteration\", \"Fold\", \"Features\" , \"Recall\", \"Precision\", 'MCC'])\n"],"metadata":{"id":"YgfcRwTV77ue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_param =  {\n","    \n","    'model__max_depth': [1,5,10, 15],\n","    'model__max_features': ['auto', 'log2' ,2],\n","    'model__n_estimators': [100, 200, 500]\n","}\n","\n","steps = [('preprocessor', preprocessor), ('model', RandomForestClassifier())]\n","rf_pipe = Pipeline(steps=steps)\n","\n","scorer = make_scorer(matthews_corrcoef)"],"metadata":{"id":"y-GnH3Co768e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"LCAeIN1xZ80M","outputId":"28132c77-85df-4fa0-fd45-e8efec76c8ac","executionInfo":{"status":"error","timestamp":1645631223876,"user_tz":-60,"elapsed":2600847,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}}},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-71-44bcc4deb043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_int\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_train_int\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#GridSearchCV over training  fold to find the optimal parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["steps=[10, 25, 50, preprocessor.fit_transform(X_train).shape[1] ] \n","n_ext=10\n","n_cv=5\n","ranking_rf = np.empty((n_ext * n_cv, preprocessor.fit_transform(X_train).shape[1]), dtype=int)\n","\n","for n in range(n_ext):\n","    \n","    skf = StratifiedKFold(n_cv, shuffle=True, random_state=n)\n","    \n","    for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n","        X_train_int, X_test_int = X_train.iloc[train_index], X_train.iloc[test_index]\n","        y_train_int, y_test_int = y_train.iloc[train_index], y_train.iloc[test_index]\n","        \n","        tuncv = StratifiedShuffleSplit(\n","                n_splits=n_cv, test_size=0.5, random_state=i\n","            )\n","        rf_grid = GridSearchCV(estimator = rf_pipe, param_grid = rf_param, scoring=scorer, cv=tuncv ,n_jobs=-1, verbose=False)\n","\n","    \n","    \n","        rf_grid.fit(X_train_int ,y_train_int) #GridSearchCV over training  fold to find the optimal parameters \n","\n","        best_model = rf_grid.best_estimator_.get_params()['model']\n","  \n","        selector_pipe.fit(X_train_int, y_train_int) #feature ranking through SelectKBest\n","\n","        #ordered list of tuples containing the index of the feature and its ranking:\n","        scores=selector.scores_[selector.get_support()]\n","        #feature=list(zip(range(scores.shape[0]), scores))\n","        #feature.sort(key=lambda x:x[1])\n","        ranking_tmp = np.argsort(scores)[::-1]\n","        ranking_rf[(n * n_cv) + i] = ranking_tmp\n","\n","    \n","        # rescaling\n","       \n","     \n","        X_train_int = preprocessor.fit_transform(X_train_int)\n","        X_test_int = preprocessor.transform(X_test_int)\n","        \n","\n","        #for step in steps:\n","          #selected_features=[tupla[0] for tupla in feature[:step]]\n","          #X_train_fs, X_test_fs = X_train_int[:, selected_features], X_test_int[:, selected_features]\n","        for j, s in enumerate(steps):\n","          v = ranking_knn[(n * n_cv) + i][:s]\n","          X_train_fs, X_test_fs = X_train_int[:, v], X_test_int[:, v]\n","          best_model.fit(X_train_fs, y_train_int)\n","      \n","      \n","\n","          #classif_rep = classification_report(y_ts, yp, output_dict=True)\n","          y_pred = best_model.predict(X_test_fs)\n","\n","\n","     \n","\n","\n","          metrics_df_rf  = metrics_df_rf.append(\n","                    {\n","                        \"Iteration\": n,\n","                        \"Fold\": i,\n","                        \"Features\": s,\n","                        \"Recall\": recall_score(y_test_int, y_pred),\n","                        \"Precision\": precision_score(y_test_int,y_pred) ,\n","                      \n","                          'MCC': matthews_corrcoef(y_test_int, y_pred),\n","                      \n","                        \n","                    },\n","                    ignore_index=True,\n","                )"]},{"cell_type":"code","source":["len(metrics_df_rf)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LafAYH5GQ9lc","executionInfo":{"status":"ok","timestamp":1645631245856,"user_tz":-60,"elapsed":216,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"14baa8d7-6694-4077-bd91-c6ee595da4f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["148"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["metrics_df_rf['Features'] = metrics_df_rf['Features'].fillna(\"all\")"],"metadata":{"id":"67gvEPLkFZ2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_df_rf.to_csv('metrics_df_rf.csv', encoding = 'utf-8-sig') "],"metadata":{"id":"n1Rb99E2FfAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics_df_rf=pd.read_csv('metrics_df_rf.csv')"],"metadata":{"id":"wPnxRLiRimPW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_rf=metrics_df_rf.groupby(['Features']).mean()"],"metadata":{"id":"X7J6iJXtFhlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNOfsKjJ_r8i"},"outputs":[],"source":["avg_df_rf.to_csv('avg_df_rf.csv', encoding = 'utf-8-sig') "]},{"cell_type":"code","source":["avg_df_rf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"PgOxaCA7Fq8O","executionInfo":{"status":"ok","timestamp":1645631267094,"user_tz":-60,"elapsed":221,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"6b1ce49c-8c83-47e8-836d-946a40751737"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-6831b116-3cd8-4c73-9a4a-5c81cf019e83\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","    </tr>\n","    <tr>\n","      <th>Features</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>10.0</th>\n","      <td>3.216216</td>\n","      <td>1.918919</td>\n","      <td>0.502134</td>\n","      <td>0.621884</td>\n","      <td>0.310525</td>\n","    </tr>\n","    <tr>\n","      <th>25.0</th>\n","      <td>3.216216</td>\n","      <td>1.918919</td>\n","      <td>0.500711</td>\n","      <td>0.646618</td>\n","      <td>0.332822</td>\n","    </tr>\n","    <tr>\n","      <th>50.0</th>\n","      <td>3.216216</td>\n","      <td>1.918919</td>\n","      <td>0.487909</td>\n","      <td>0.659329</td>\n","      <td>0.340846</td>\n","    </tr>\n","    <tr>\n","      <th>67.0</th>\n","      <td>3.216216</td>\n","      <td>1.918919</td>\n","      <td>0.481271</td>\n","      <td>0.666049</td>\n","      <td>0.343064</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6831b116-3cd8-4c73-9a4a-5c81cf019e83')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6831b116-3cd8-4c73-9a4a-5c81cf019e83 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6831b116-3cd8-4c73-9a4a-5c81cf019e83');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["          Iteration      Fold    Recall  Precision       MCC\n","Features                                                    \n","10.0       3.216216  1.918919  0.502134   0.621884  0.310525\n","25.0       3.216216  1.918919  0.500711   0.646618  0.332822\n","50.0       3.216216  1.918919  0.487909   0.659329  0.340846\n","67.0       3.216216  1.918919  0.481271   0.666049  0.343064"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["#append confidence intervals\n","cols=['Recall' ,'Precision', 'MCC']\n","\n","for col in cols:\n","  avg_df_rf[col]=metrics_df_rf.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).value)\n","  avg_df_rf['Lower Bound '+ col]=metrics_df_rf.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).lower_bound)\n","  avg_df_rf['Upper Bound '+ col]=metrics_df_rf.groupby('Features')[col].apply(lambda x:bs.bootstrap(x.values ,stat_func=bs_stats.mean ).upper_bound)"],"metadata":{"id":"kbmkvkvgFxFZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_df_rf.to_csv('CI_df_rf.csv', encoding = 'utf-8-sig') "],"metadata":{"id":"MJU0dI37F2Y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CI_df_rf = pd.read_csv('CI_df_rf.csv')\n","CI_df_rf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273},"id":"pMiiJ-vmF6Fs","executionInfo":{"status":"ok","timestamp":1645631285116,"user_tz":-60,"elapsed":6,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"59983a28-c7cc-4ee0-dd4c-1502812c5f96"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-7dfd6de0-cd8f-4020-96d8-4d49e1d04d4a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Features</th>\n","      <th>Iteration</th>\n","      <th>Fold</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>MCC</th>\n","      <th>Lower Bound Recall</th>\n","      <th>Upper Bound Recall</th>\n","      <th>Lower Bound Precision</th>\n","      <th>Upper Bound Precision</th>\n","      <th>Lower Bound MCC</th>\n","      <th>Upper Bound MCC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10.0</td>\n","      <td>3.216216</td>\n","      <td>1.918919</td>\n","      <td>0.502134</td>\n","      <td>0.621884</td>\n","      <td>0.310525</td>\n","      <td>0.477952</td>\n","      <td>0.526790</td>\n","      <td>0.600790</td>\n","      <td>0.640898</td>\n","      <td>0.283657</td>\n","      <td>0.336176</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>25.0</td>\n","      <td>3.216216</td>\n","      <td>1.918919</td>\n","      <td>0.500711</td>\n","      <td>0.646618</td>\n","      <td>0.332822</td>\n","      <td>0.478900</td>\n","      <td>0.522997</td>\n","      <td>0.624652</td>\n","      <td>0.668461</td>\n","      <td>0.307025</td>\n","      <td>0.358565</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>50.0</td>\n","      <td>3.216216</td>\n","      <td>1.918919</td>\n","      <td>0.487909</td>\n","      <td>0.659329</td>\n","      <td>0.340846</td>\n","      <td>0.464675</td>\n","      <td>0.510669</td>\n","      <td>0.636963</td>\n","      <td>0.680740</td>\n","      <td>0.314149</td>\n","      <td>0.367175</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>67.0</td>\n","      <td>3.216216</td>\n","      <td>1.918919</td>\n","      <td>0.481271</td>\n","      <td>0.666049</td>\n","      <td>0.343064</td>\n","      <td>0.453770</td>\n","      <td>0.508298</td>\n","      <td>0.641180</td>\n","      <td>0.690823</td>\n","      <td>0.312675</td>\n","      <td>0.373849</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7dfd6de0-cd8f-4020-96d8-4d49e1d04d4a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7dfd6de0-cd8f-4020-96d8-4d49e1d04d4a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7dfd6de0-cd8f-4020-96d8-4d49e1d04d4a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Features  Iteration  ...  Lower Bound MCC  Upper Bound MCC\n","0      10.0   3.216216  ...         0.283657         0.336176\n","1      25.0   3.216216  ...         0.307025         0.358565\n","2      50.0   3.216216  ...         0.314149         0.367175\n","3      67.0   3.216216  ...         0.312675         0.373849\n","\n","[4 rows x 12 columns]"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["mcc_results.append (('Random Forest',\n","                 CI_df_rf[  (CI_df_rf.MCC == CI_df_rf.MCC.values.max())  ] ['MCC'].iloc[0],\n","                 CI_df_rf[  (CI_df_rf.MCC == CI_df_rf.MCC.values.max())  ]  ['Features'].iloc[0] ))"],"metadata":{"id":"YQyrbC48rpwt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CONCLUSIONS"],"metadata":{"id":"yaqZxvVas-t_"}},{"cell_type":"code","source":["mcc_results "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dpso2I9EtApW","executionInfo":{"status":"ok","timestamp":1645631600227,"user_tz":-60,"elapsed":210,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"21d7bf75-5b56-42c6-a5b0-bed8ae4640bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Logistic Regression ', 0.2900961417417739, 10.0),\n"," ('KNN', 0.2463404281570071, 10.0),\n"," ('Random Forest', 0.3430637966324857, 67.0),\n"," ('Decision Tree ', 0.318485976770124, '10.0')]"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["#final model\n","from utilities import *\n","print(final_model(mcc_results))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHiaKZRTWedt","executionInfo":{"status":"ok","timestamp":1645632749468,"user_tz":-60,"elapsed":223,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"065dbc1c-b51e-4149-d3b0-b070a2e3583f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'model': 'Random Forest', 'metric_value': 0.3430637966324857, 'features': 67.0}\n"]}]},{"cell_type":"markdown","source":["VALIDATION EXPERIMENT\n"],"metadata":{"id":"ssTSyje0SlTr"}},{"cell_type":"code","source":["rf_param =  {\n","    \n","    'model__max_depth': [1,5,10, 15],\n","    'model__max_features': ['auto', 'log2' ,2],\n","    'model__n_estimators': [100, 200, 500]\n","}\n","\n","steps = [('preprocessor', preprocessor), ('model', RandomForestClassifier())]\n","rf_pipe = Pipeline(steps=steps)\n","\n","scorer = make_scorer(matthews_corrcoef)\n","cv = StratifiedShuffleSplit(\n","                n_splits=n_cv, test_size=0.5, random_state=123\n","            )\n","rf_grid = GridSearchCV(estimator = rf_pipe, param_grid = rf_param, cv=cv,  scoring=make_scorer(matthews_corrcoef) ,n_jobs=-1, verbose=False)"],"metadata":{"id":"HM3TmJ0gSntG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_grid.fit(X_train, y_train) #GridSearchCV over training  fold to find the optimal parameters on the entire train and test set  \n","\n","y_pred=rf_grid.predict(X_test)\n","\n","print('mcc%.3f' %matthews_corrcoef(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G5oNBoTtS0Te","executionInfo":{"status":"ok","timestamp":1645632128625,"user_tz":-60,"elapsed":73150,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"9699f791-69a5-4bcb-fbf7-c610855917f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mcc0.285\n"]}]},{"cell_type":"markdown","source":["Compare this feature selection with the one operated by the authors of the study:"],"metadata":{"id":"7Zwk7y7aUzuu"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":91},"id":"c_y7sDnsU3Ck","executionInfo":{"status":"ok","timestamp":1645632273342,"user_tz":-60,"elapsed":7808,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"520fd3d6-0960-44f4-ebd8-8954daa6b44a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-02225576-c349-4629-b428-cb6d04bb95c5\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-02225576-c349-4629-b428-cb6d04bb95c5\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving df_newfilter.xlsx to df_newfilter.xlsx\n"]}]},{"cell_type":"code","source":["df_newfilter= pd.read_excel('df_newfilter.xlsx')"],"metadata":{"id":"e7Lh79TOU74D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = df_newfilter.drop(['ult_tsa:placca_tot','ult_tsa:placca_dx_recod_1', 'ult_tsa:placca_sx_recod_1', 'ult_tsa:placca_1','ult_tsa:IMT_CC_max_sx_1', 'ult_tsa:IMT_CC_max_dx_1', 'ult_tsa:IMT_CC_medio_round_sx_1', 'ult_tsa:IMT_CC_medio_round_dx_1', 'ult_tsa:IMT_CC_medio_round_mean_1'], axis=1)"],"metadata":{"id":"YSKr08TnU-yg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y=df_newfilter['ult_tsa:placca_tot']\n","# summarize class distribution\n","counter = Counter(y)\n","print(counter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZlBBKepVEuL","executionInfo":{"status":"ok","timestamp":1645632427535,"user_tz":-60,"elapsed":6,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"8a31f6ae-8914-47e0-db08-afeea961f305"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Counter({0: 557, 1: 374})\n"]}]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=42)\n","\n","num_pipeline = Pipeline([\n","        ('scaler', StandardScaler()),\n","    ])\n","\n","cat_pipeline = Pipeline([\n","        ('ohe', OneHotEncoder(handle_unknown = 'ignore')),\n","    ])\n","num_attribs = list( X_train.select_dtypes(include=['int64', 'float64']).columns)\n","cat_attribs = list( X_train.select_dtypes(include='object').columns)\n","\n","preprocessor = ColumnTransformer([\n","        (\"num\", num_pipeline, num_attribs),\n","        (\"cat\", cat_pipeline, cat_attribs),\n","    ])"],"metadata":{"id":"QiJIJMrDVI5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_param =  {\n","    \n","    'model__max_depth': [1,5,10, 15],\n","    'model__max_features': ['auto', 'log2' ,2],\n","    'model__n_estimators': [100, 200, 500]\n","}\n","\n","steps = [('preprocessor', preprocessor), ('model', RandomForestClassifier())]\n","rf_pipe = Pipeline(steps=steps)\n","\n","scorer = make_scorer(matthews_corrcoef)\n","cv = StratifiedShuffleSplit(\n","                n_splits=n_cv, test_size=0.5, random_state=123\n","            )\n","rf_grid = GridSearchCV(estimator = rf_pipe, param_grid = rf_param, cv=cv,  scoring=make_scorer(matthews_corrcoef) ,n_jobs=-1, verbose=False)"],"metadata":{"id":"brmYpQKWVacW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_grid.fit(X_train, y_train) #GridSearchCV over training  fold to find the optimal parameters on the entire train and test set  \n","\n","y_pred=rf_grid.predict(X_test)\n","\n","print('mcc according to this new filtering :%3f' %matthews_corrcoef(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSeLmtaIVPTE","executionInfo":{"status":"ok","timestamp":1645632513253,"user_tz":-60,"elapsed":70663,"user":{"displayName":"Tamara Rigo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00468278005850858086"}},"outputId":"832a412e-3432-4080-a22a-8c76a4f08c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mcc according to this new filtering :0.270641\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"G75Q6HFQo2vK"}}]}